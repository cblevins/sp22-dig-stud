{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6: Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os and string packages here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply Python to our sample text file to try and generate some metadata about it. Take the filename of the text file that you downloaded and then use that filename to make new variables for:\n",
    "\n",
    "- Full date (as a string in `YYYY-MM-DD` format)\n",
    "- Year (as an integer)\n",
    "- Month (as an integer)\n",
    "- Day (as an integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here for generating metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to bulk download a bunch of OCR text files from the Chronicling America database. Rather than download each file individually, we're going to take a shortcut by using a pre-made Python script that Brandon Locke created: <https://twitter.com/brandontlocke/status/1045659312851505152> and <https://github.com/brandontlocke/downloadingamerica>. This allows us to use the search function in the Chronicling America database and then download all the text files that were returned from that search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Locke's script by going to <https://github.com/brandontlocke/downloadingamerica> and clicking the green Clone or Download button. Download it as a zip file, unzip the folder, adn move it inside your `week-06` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually run Locke's script. When it asks for a search string, let's use one that will generate search results for all newspapers that were printed on Christmas Day in 1919 in Arizona and Washington, DC:\n",
    "\n",
    "`https://chroniclingamerica.loc.gov/search/pages/results/?state=Arizona&state=District+of+Columbia&dateFilterType=range&date1=12/25/1919&date2=12/25/1919&ortext=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homedir=os.getcwd() #setting your home directory, because you need to change directories to easily use the magic %run command\n",
    "os.chdir(\"downloadingamerica-master\") #change to directory containing the python script\n",
    "%run downloadingamerica #run the script - this will \n",
    "os.chdir(homedir) #move back up to the home directory so you can re-run things without having a relative file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look in the folder that contains Locke's script, you should see a bunch of new text files - this is the magic of coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle the Data\n",
    "\n",
    "In this section we'll take all of these text files of newspaper pages and put them into various data structures and do some basic \"cleaning up\" of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of filenames from our directory of downloaded files, then create a list called `page_list` where each item in the list is the text from a different file (ie. a page of a newspaper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to create a `dictionary` called `page_dict`. This is a differnet sort of data structure from a `list`. Think of this as an actual dictionary: \n",
    "- **key** = \"elephant\" \n",
    "- **value** = \"a heavy plant-eating mammal with a prehensile trunk, long curved ivory tusks, and large ears, native to Africa and southern Asia. It is the largest living land animal.\"\n",
    "\n",
    "Here's an example dictionary with two entries, one for elephant and one for giraffe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_dict = {'elephant': 'a heavy plant-eating mammal with a prehensile trunk', 'giraffe': 'a highly overrated and unnatractive animal with a long neck'}\n",
    "print(animal_dict['elephant'])\n",
    "print(animal_dict['giraffe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we're going to make a dictionary where each entry in the dictionary consists of a different page of newspaper. So the \"key\" for each entry will be the filename, and its \"value\" as the contents of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dict={}\n",
    "#for loop below to populate page_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at our two different structures - they basically contain the same thing, with each entry consisting of the text of a single page of page. But the `list` version is arranged sequentially according to how they were arranged in the folder on your computer. We don't have any information about the newspaper itself (title, date, etc.) A `dictionary`, on the other hand, stores this information in the filename which is used as the **key**. So much like a normal dictionary, we can actually look up a particular page using its filename and access its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first 100 characters of our fifth newspaper page in our list\")\n",
    "#print statement here...\n",
    "print(\"_____\")\n",
    "\n",
    "print(\"The first 100 characters of a particular newspaper\")\n",
    "#print statement here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to create one giant list of ALL the words that appear in ALL the pages of newspapers, called `all_words_list`. We're also going to clean up the words to remove punctuation marks, put them all into lowercase, get rid of extra whitespce characters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is a list of punctuation characters that we're going to remove:\", string.punctuation)\n",
    "# w = w.translate(str.maketrans('','',string.punctuation)) #this is a line of code that removes punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first 10 words contained in our giant list of words:\")\n",
    "#print statement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Word Frequencies \n",
    "\n",
    "Create a `dictionary` called `words_freq` in which the **key** is going to be a word (ex. \"christmas\") and the **value** is going to be how many times that word appeared, which we're going to calculate as we go through every word in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq={}\n",
    "#now populate words_freq with a for loop and if statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times does \"Christmas\" appear? We can access the **value** of a paritcular entry in our dictionary by using its **key**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately dictionaries aren't meant to be sorted sequentially. We can do a work-around to arrange the entries in descending order by the word's frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples so we can sort our dictionary items   \n",
    "listofTuples = sorted(words_freq.items() ,  key=lambda x: x[1], reverse=True)\n",
    "#show us the most frequent words\n",
    "for p in listofTuples[0:40]:\n",
    "    print(p[0], \":\", p[1])\n",
    "print(\"The most common word is:\", listofTuples[0][0], \", and it occurs:\", listofTuples[0][1], \"times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this is a really useless list. You can see it contains really common words (ex. \"the\" \"and\") that aren't very interetsting. Common words are often called \"stop words\" and there are standardized lists of these that we can use to remove them. So we can re-run this but this time remove these words, along with numbers like 1, 2, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq={}\n",
    "infile=open(\"stop-word-list.txt\", mode='r', encoding='utf-8')\n",
    "stopwords=infile.read()\n",
    "\n",
    "for word in all_words_list:\n",
    "    if word in stopwords: # if the word is in our list of stop words, we don't want to add it\n",
    "        continue #this means \"go onto the next item in our for loop without doing the other commands\"\n",
    "    if word.isalpha() == False: #look at the word and if it is NOT in the alphabet - ie. NOT a letter a-z - then skip it\n",
    "        continue \n",
    "    if word in words_freq:\n",
    "        words_freq[word]=words_freq[word]+1\n",
    "    else:\n",
    "        words_freq[word]=1\n",
    "\n",
    "# Create a list of tuples so we can sort our dictionary items   \n",
    "listofTuples = sorted(words_freq.items() ,  key=lambda x: x[1], reverse=True)\n",
    "#show us the top 40 most frequent words\n",
    "for p in listofTuples[0:40]:\n",
    "    print(p[0], \":\", p[1])\n",
    "print(\"The most common word is:\", listofTuples[0][0], \", and it occurs:\", listofTuples[0][1], \"times.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
